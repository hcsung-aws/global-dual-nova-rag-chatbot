"""
ì´ì¤‘ ì–¸ì–´ ì‘ë‹µ ìƒì„± ëª¨ë“ˆ

ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë¶„ë¦¬ëœ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
- generate_dual_answer: ë“€ì–¼ ëª¨ë¸ì„ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±
- generate_dual_language_response: ì˜ì–´ ì‚¬ìš©ìë¥¼ ìœ„í•œ ì´ì¤‘ ì–¸ì–´ ë‹µë³€ ìƒì„±
- create_prompts: ê° ëª¨ë¸ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„±
"""

import streamlit as st
import time
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any, Optional, Tuple

from src.core.prompt_generator import PromptFactory
from src.core.streaming_handler import StreamingHandler


class DualResponseGenerator:
    """ì´ì¤‘ ì–¸ì–´ ì‘ë‹µ ìƒì„±ê¸°
    
    ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë¶„ë¦¬ëœ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    UI ë¡œì§ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ë¶„ë¦¬í•˜ì—¬ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ì„±ê³¼ ì¬ì‚¬ìš©ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    """
    
    def __init__(self, streaming_handler: StreamingHandler):
        """
        Args:
            streaming_handler: ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” í•¸ë“¤ëŸ¬
        """
        self.streaming_handler = streaming_handler
    
    def create_prompts(self, user_query: str, context_docs: List[Dict], 
                      conversation_history: List[Dict], user_language: str = "Korean") -> Tuple[str, str]:
        """ê° ëª¨ë¸ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ ìƒì„± (ë‹¤êµ­ì–´ ì§€ì›, ìºì‹± ìµœì í™”)
        
        Args:
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
            context_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬
            user_language: ì‚¬ìš©ì ì–¸ì–´ ("Korean" ë˜ëŠ” "English")
            
        Returns:
            Tuple[str, str]: (micro_prompt, pro_prompt)
        """
        # ìºì‹± ê°€ëŠ¥í•œ prefix ìƒì„±
        cached_prefix = PromptFactory.create_answer_prompt(user_language=user_language)
        
        # ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
        context = ""
        if context_docs:
            context = "\n\n".join([f"ì œëª©: {doc['title']}\në‚´ìš©: {doc['content'][:1000]}" for doc in context_docs])
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¤€ë¹„
        history_context = ""
        if conversation_history:
            recent_history = conversation_history[-4:]
            history_parts = []
            for i in range(0, len(recent_history), 2):
                if i + 1 < len(recent_history):
                    history_parts.append(f"ì´ì „ ì§ˆë¬¸: {recent_history[i]['content']}\nì´ì „ ë‹µë³€: {recent_history[i + 1]['content']}")
            if history_parts:
                history_context = f"ì´ì „ ëŒ€í™”:\n{chr(10).join(history_parts)}\n\n"
        
        # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸ ì¡°ì • (ìºì‹±ëœ prefix í™œìš©)
        if user_language == "English":
            # ì˜ì–´ ì‚¬ìš©ìë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸
            micro_prompt = f"""{cached_prefix}

## Current Task: Provide Concise Overview

{history_context}Based on the following documents, please provide a concise overview:

{context}

Question: {user_query}

Requirements:
- Provide key information in 1-2 paragraphs clearly and concisely
- End with complete sentences
- Finish with a simple connecting phrase like "Let me provide more details."
- Respond in English in a friendly manner
- Keep it brief to avoid incomplete responses
- Use the game terminology glossary above for accurate character and term recognition

Answer:"""

            pro_prompt = f"""{cached_prefix}

## Current Task: Provide Detailed Analysis

{history_context}I previously provided a basic overview for "{user_query}". 
Now, please provide more detailed and professional analysis based on the following documents:

{context}

Question: {user_query}

Requirements:
- Start naturally as an extension of the previous answer (e.g., "Looking more specifically...", "To analyze this in more detail...")
- Include specific examples, pros/cons, use cases, and comparative analysis
- Use markdown for structured and organized answers
- Provide practical and professional insights
- Respond in English in a friendly and professional manner
- Base your answer accurately and in detail on the provided documents
- Explain step-by-step when necessary
- Consider previous conversation context
- Use the game terminology glossary above for accurate character and term recognition

Answer:"""
        else:
            # í•œêµ­ì–´ ì‚¬ìš©ìë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ (ê¸°ì¡´)
            micro_prompt = f"""{cached_prefix}

## í˜„ì¬ ì‘ì—…: ê°„ê²°í•œ ê°œìš” ì œê³µ

{history_context}ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ëŒ€í•œ ê°„ê²°í•œ ê°œìš”ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

{context}

ì§ˆë¬¸: {user_query}

ìš”êµ¬ì‚¬í•­:
- í•µì‹¬ ë‚´ìš©ì„ 1-2ê°œ ë¬¸ë‹¨ìœ¼ë¡œ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ì„¤ëª…
- ë°˜ë“œì‹œ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ë§ˆë¬´ë¦¬í•  ê²ƒ
- ë§ˆì§€ë§‰ì— "ë” êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤." ê°™ì€ ê°„ë‹¨í•œ ì—°ê²° ë¬¸êµ¬ë¡œ ë§ˆë¬´ë¦¬
- í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
- ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì—¬ ë‹µë³€ì´ ì¤‘ê°„ì— ëŠì–´ì§€ì§€ ì•Šë„ë¡ í•´ì£¼ì„¸ìš”
- ìœ„ì˜ ê²Œì„ ìš©ì–´ ë‹¨ì–´ì¥ì„ í™œìš©í•˜ì—¬ ì •í™•í•œ ìºë¦­í„° ë° ìš©ì–´ ì¸ì‹

ë‹µë³€:"""

            pro_prompt = f"""{cached_prefix}

## í˜„ì¬ ì‘ì—…: ìƒì„¸í•œ ë¶„ì„ ì œê³µ

{history_context}ì•ì„œ "{user_query}"ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ê°œìš”ë¥¼ ì œê³µí–ˆìŠµë‹ˆë‹¤. 
ì´ì œ ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¸ê³ í•˜ì—¬ ê·¸ ë‚´ìš©ì— ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ì„œ ë” ìƒì„¸í•˜ê³  ì „ë¬¸ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”:

{context}

ì§ˆë¬¸: {user_query}

ìš”êµ¬ì‚¬í•­:
- ì•ì„  ë‹µë³€ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì—°ì¥ì„ ìƒì—ì„œ ì‹œì‘ (ì˜ˆ: "ë” êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³´ë©´...", "ì´ë¥¼ ë” ìì„¸íˆ ë¶„ì„í•˜ë©´..." ë“±)
- êµ¬ì²´ì ì¸ ì˜ˆì‹œ, ì¥ë‹¨ì , ì‚¬ìš© ì‚¬ë¡€, ë¹„êµ ë¶„ì„ì„ í¬í•¨í•œ ì‹¬í™” ë‚´ìš©
- ë§ˆí¬ë‹¤ìš´ì„ í™œìš©í•œ ì²´ê³„ì ì´ê³  êµ¬ì¡°í™”ëœ ë‹µë³€
- ì‹¤ë¬´ì ì´ê³  ì „ë¬¸ì ì¸ ê´€ì ì—ì„œì˜ ê¹Šì´ ìˆëŠ” ì„¤ëª…
- í•œêµ­ì–´ë¡œ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
- ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
- í•„ìš”ì‹œ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”
- ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
- ìœ„ì˜ ê²Œì„ ìš©ì–´ ë‹¨ì–´ì¥ì„ í™œìš©í•˜ì—¬ ì •í™•í•œ ìºë¦­í„° ë° ìš©ì–´ ì¸ì‹

ë‹µë³€:"""

        return micro_prompt, pro_prompt
    
    def generate_dual_language_response(self, query: str, context_docs: List[Dict], 
                                      conversation_history: List[Dict],
                                      micro_prompt_ko: str, pro_prompt_ko: str, 
                                      micro_prompt_en: str, pro_prompt_en: str) -> str:
        """ì˜ì–´ ì‚¬ìš©ìë¥¼ ìœ„í•œ ì´ì¤‘ ì–¸ì–´ ë‹µë³€ ìƒì„± - GitHub ì›ë³¸ ë°©ì‹ ì™„ì „ ì ìš©
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬
            micro_prompt_ko: í•œêµ­ì–´ Micro í”„ë¡¬í”„íŠ¸
            pro_prompt_ko: í•œêµ­ì–´ Pro í”„ë¡¬í”„íŠ¸
            micro_prompt_en: ì˜ì–´ Micro í”„ë¡¬í”„íŠ¸
            pro_prompt_en: ì˜ì–´ Pro í”„ë¡¬í”„íŠ¸
            
        Returns:
            str: ìƒì„±ëœ ì´ì¤‘ ì–¸ì–´ ì‘ë‹µ
        """
        # ì‘ë‹µì„ ì €ì¥í•  ì»¨í…Œì´ë„ˆ
        response_container = st.empty()
        
        # 1ë‹¨ê³„: í•œêµ­ì–´ ë‹µë³€ ìƒì„± (GitHub ì›ë³¸ê³¼ ë™ì¼í•œ ë³‘ë ¬ ì‹¤í–‰)
        st.caption("ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë‹µë³€ ìƒì„± ì¤‘ (ë‹´ë‹¹ì í™•ì¸ìš©) - GitHub ì›ë³¸ ë°©ì‹ ë³‘ë ¬ ì²˜ë¦¬...")
        
        # Pro ëª¨ë¸ì˜ ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ ì €ì¥í•  ë²„í¼ (GitHub ì›ë³¸ê³¼ ë™ì¼)
        korean_pro_buffer = []
        korean_pro_complete = False
        
        def collect_korean_pro_stream():
            """í•œêµ­ì–´ Pro ëª¨ë¸ì˜ ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ ë²„í¼ì— ìˆ˜ì§‘ (StreamingHandler ì‚¬ìš©)"""
            nonlocal korean_pro_buffer, korean_pro_complete
            completion_flag = [False]
            # StreamingHandlerë¥¼ ì‚¬ìš©í•˜ì—¬ ë²„í¼ ìˆ˜ì§‘
            self.streaming_handler.collect_stream_to_buffer(
                'amazon.nova-pro-v1:0', pro_prompt_ko, korean_pro_buffer, completion_flag,
                max_tokens=2048, temperature=0.5
            )
            korean_pro_complete = completion_flag[0]
        
        # GitHub ì›ë³¸ê³¼ ë™ì¼í•œ ThreadPoolExecutor ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=2) as executor:
            # Pro ëª¨ë¸ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ (GitHub ì›ë³¸ê³¼ ë™ì¼)
            future_korean_pro = executor.submit(collect_korean_pro_stream)
            
            # 1ë‹¨ê³„: Nova Micro ìŠ¤íŠ¸ë¦¬ë° (StreamingHandler ì‚¬ìš©)
            korean_micro_response = self.streaming_handler.stream_with_realtime_display(
                model_id='amazon.nova-micro-v1:0',
                prompt=micro_prompt_ko,
                display_callback=lambda text: response_container.markdown(f"## ğŸ“‹ í•œêµ­ì–´ ë‹µë³€ (ë‹´ë‹¹ì í™•ì¸ìš©)\n{text}"),
                max_tokens=1024,
                temperature=0.3,
                typing_delay=0.02
            )
            
            # 2ë‹¨ê³„: Pro ëª¨ë¸ ê²°ê³¼ ì¶œë ¥ (GitHub ì›ë³¸ê³¼ ë™ì¼í•œ ë²„í¼ ì²˜ë¦¬)
            st.caption("ğŸ” í•œêµ­ì–´ ìƒì„¸ ë¶„ì„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²° ì¤‘...")
            
            buffer_index = 0
            korean_current_display = f"## ğŸ“‹ í•œêµ­ì–´ ë‹µë³€ (ë‹´ë‹¹ì í™•ì¸ìš©)\n{korean_micro_response}\n\n---\n\n**ğŸ” Nova Pro ìƒì„¸ ë¶„ì„:**\n\n"
            
            # GitHub ì›ë³¸ê³¼ ë™ì¼í•œ ë²„í¼ ì‹¤ì‹œê°„ ì¶œë ¥ ë°©ì‹
            while not korean_pro_complete or buffer_index < len(korean_pro_buffer):
                # ë²„í¼ì— ìƒˆë¡œìš´ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¶œë ¥
                while buffer_index < len(korean_pro_buffer):
                    chunk = korean_pro_buffer[buffer_index]
                    korean_current_display += chunk
                    response_container.markdown(korean_current_display + "â–Œ")
                    time.sleep(0.01)  # GitHub ì›ë³¸ê³¼ ë™ì¼í•œ ë¹ ë¥¸ íƒ€ì´í•‘ íš¨ê³¼
                    buffer_index += 1
                
                # Pro ëª¨ë¸ì´ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì ì‹œ ëŒ€ê¸° (GitHub ì›ë³¸ê³¼ ë™ì¼)
                if not korean_pro_complete:
                    time.sleep(0.05)
            
            # í•œêµ­ì–´ ìµœì¢… í‘œì‹œ (ì»¤ì„œ ì œê±°)
            response_container.markdown(korean_current_display)
            
            # Pro ì‘ì—… ì™„ë£Œ ëŒ€ê¸° (GitHub ì›ë³¸ê³¼ ë™ì¼)
            future_korean_pro.result()
            korean_final = korean_current_display
        
        # 2ë‹¨ê³„: ì˜ì–´ ë‹µë³€ ìƒì„± (GitHub ì›ë³¸ê³¼ ë™ì¼í•œ ë³‘ë ¬ ì‹¤í–‰)
        st.caption("ğŸ‡ºğŸ‡¸ ì˜ì–´ ë‹µë³€ ìƒì„± ì¤‘ (ê³ ê°ìš©) - GitHub ì›ë³¸ ë°©ì‹ ë³‘ë ¬ ì²˜ë¦¬...")
        
        # Pro ëª¨ë¸ì˜ ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ ì €ì¥í•  ë²„í¼ (GitHub ì›ë³¸ê³¼ ë™ì¼)
        english_pro_buffer = []
        english_pro_complete = False
        
        def collect_english_pro_stream():
            """ì˜ì–´ Pro ëª¨ë¸ì˜ ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ ë²„í¼ì— ìˆ˜ì§‘ (StreamingHandler ì‚¬ìš©)"""
            nonlocal english_pro_buffer, english_pro_complete
            completion_flag = [False]
            # StreamingHandlerë¥¼ ì‚¬ìš©í•˜ì—¬ ë²„í¼ ìˆ˜ì§‘
            self.streaming_handler.collect_stream_to_buffer(
                'amazon.nova-pro-v1:0', pro_prompt_en, english_pro_buffer, completion_flag,
                max_tokens=2048, temperature=0.5
            )
            english_pro_complete = completion_flag[0]
        
        # GitHub ì›ë³¸ê³¼ ë™ì¼í•œ ThreadPoolExecutor ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=2) as executor:
            # Pro ëª¨ë¸ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ (GitHub ì›ë³¸ê³¼ ë™ì¼)
            future_english_pro = executor.submit(collect_english_pro_stream)
            
            # 1ë‹¨ê³„: Nova Micro ìŠ¤íŠ¸ë¦¬ë° (StreamingHandler ì‚¬ìš©)
            english_micro_response = self.streaming_handler.stream_with_realtime_display(
                model_id='amazon.nova-micro-v1:0',
                prompt=micro_prompt_en,
                display_callback=lambda text: response_container.markdown(f"""{korean_final}

---

## ğŸŒ English Response (For Customer)
{text}"""),
                max_tokens=1024,
                temperature=0.3,
                typing_delay=0.02
            )
            
            # 2ë‹¨ê³„: Pro ëª¨ë¸ ê²°ê³¼ ì¶œë ¥ (GitHub ì›ë³¸ê³¼ ë™ì¼í•œ ë²„í¼ ì²˜ë¦¬)
            st.caption("ğŸ” ì˜ì–´ ìƒì„¸ ë¶„ì„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²° ì¤‘...")
            
            buffer_index = 0
            english_current_display = f"""{korean_final}

---

## ğŸŒ English Response (For Customer)
{english_micro_response}

---

**ğŸ” Nova Pro Detailed Analysis:**

"""
            
            # GitHub ì›ë³¸ê³¼ ë™ì¼í•œ ë²„í¼ ì‹¤ì‹œê°„ ì¶œë ¥ ë°©ì‹
            while not english_pro_complete or buffer_index < len(english_pro_buffer):
                # ë²„í¼ì— ìƒˆë¡œìš´ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¶œë ¥
                while buffer_index < len(english_pro_buffer):
                    chunk = english_pro_buffer[buffer_index]
                    english_current_display += chunk
                    response_container.markdown(english_current_display + "â–Œ")
                    time.sleep(0.01)  # GitHub ì›ë³¸ê³¼ ë™ì¼í•œ ë¹ ë¥¸ íƒ€ì´í•‘ íš¨ê³¼
                    buffer_index += 1
                
                # Pro ëª¨ë¸ì´ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì ì‹œ ëŒ€ê¸° (GitHub ì›ë³¸ê³¼ ë™ì¼)
                if not english_pro_complete:
                    time.sleep(0.05)
            
            # ìµœì¢… í‘œì‹œ (ì»¤ì„œ ì œê±°)
            response_container.markdown(english_current_display)
            
            # Pro ì‘ì—… ì™„ë£Œ ëŒ€ê¸° (GitHub ì›ë³¸ê³¼ ë™ì¼)
            future_english_pro.result()
            
            return english_current_display
    
    def generate_dual_answer(self, query: str, context_docs: List[Dict], 
                           conversation_history: List[Dict], user_language: str = "Korean") -> str:
        """ë“€ì–¼ ëª¨ë¸ì„ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„± (ì™„ì „í•œ ë‹¤êµ­ì–´ ì§€ì›)
        
        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            context_docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡
            conversation_history: ëŒ€í™” íˆìŠ¤í† ë¦¬
            user_language: ì‚¬ìš©ì ì–¸ì–´ ("Korean" ë˜ëŠ” "English")
            
        Returns:
            str: ìƒì„±ëœ ë‹µë³€
        """
        if not context_docs:
            if user_language == "English":
                return "Sorry, I couldn't find relevant documents. Please try searching with different keywords or rephrase your question."
            else:
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì‹œê±°ë‚˜ ì§ˆë¬¸ì„ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”."
        
        # ì‚¬ìš©ì ì–¸ì–´ì— ë”°ë¼ í”„ë¡¬í”„íŠ¸ ìƒì„±
        if user_language == "English":
            # ì˜ì–´ ì‚¬ìš©ì: í•œêµ­ì–´ ë‹µë³€ + ì˜ì–´ ë‹µë³€ ëª¨ë‘ ìƒì„±
            # 1ë‹¨ê³„: í•œêµ­ì–´ë¡œ ë‹µë³€ ìƒì„± (ë‹´ë‹¹ì í™•ì¸ìš©)
            micro_prompt_ko, pro_prompt_ko = self.create_prompts(query, context_docs, conversation_history, "Korean")
            
            # 2ë‹¨ê³„: ì˜ì–´ë¡œ ë‹µë³€ ìƒì„± (ê³ ê°ìš©)
            micro_prompt_en, pro_prompt_en = self.create_prompts(query, context_docs, conversation_history, "English")
            
            return self.generate_dual_language_response(query, context_docs, conversation_history, 
                                                      micro_prompt_ko, pro_prompt_ko, 
                                                      micro_prompt_en, pro_prompt_en)
        else:
            # í•œêµ­ì–´ ì‚¬ìš©ì: GitHub ì›ë³¸ê³¼ 100% ë™ì¼í•œ ë³‘ë ¬ ì‹¤í–‰
            micro_prompt, pro_prompt = self.create_prompts(query, context_docs, conversation_history, "Korean")
            
            # ì‘ë‹µì„ ì €ì¥í•  ì»¨í…Œì´ë„ˆ
            response_container = st.empty()
            
            # StreamingHandlerë¥¼ ì‚¬ìš©í•œ ë³‘ë ¬ ìŠ¤íŠ¸ë¦¬ë° (ê¸°ì¡´ ë¡œì§ í†µí•©)
            pro_buffer = []
            pro_complete = False
            
            def collect_pro_stream():
                """Pro ëª¨ë¸ì˜ ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ë¥¼ ë²„í¼ì— ìˆ˜ì§‘ (StreamingHandler ì‚¬ìš©)"""
                nonlocal pro_buffer, pro_complete
                completion_flag = [False]
                # StreamingHandlerë¥¼ ì‚¬ìš©í•˜ì—¬ ë²„í¼ ìˆ˜ì§‘
                self.streaming_handler.collect_stream_to_buffer(
                    'amazon.nova-pro-v1:0', pro_prompt, pro_buffer, completion_flag,
                    max_tokens=2048, temperature=0.5
                )
                pro_complete = completion_flag[0]
            
            # GitHub ì›ë³¸ê³¼ ë™ì¼í•œ ThreadPoolExecutor ë³‘ë ¬ ì‹¤í–‰
            with ThreadPoolExecutor(max_workers=2) as executor:
                st.caption("ğŸš€ StreamingHandler í†µí•©: Nova Micro + Pro ë³‘ë ¬ ì‹¤í–‰...")
                
                # Pro ëª¨ë¸ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹œì‘ (GitHub ì›ë³¸ê³¼ ë™ì¼)
                future_pro = executor.submit(collect_pro_stream)
                
                # 1ë‹¨ê³„: Nova Micro ìŠ¤íŠ¸ë¦¬ë° (StreamingHandler ì‚¬ìš©)
                full_micro_response = self.streaming_handler.stream_with_realtime_display(
                    model_id='amazon.nova-micro-v1:0',
                    prompt=micro_prompt,
                    display_callback=lambda text: response_container.markdown(text),
                    max_tokens=1024,
                    temperature=0.3,
                    typing_delay=0.02
                )
                
                # 2ë‹¨ê³„: Pro ëª¨ë¸ ê²°ê³¼ ì¶œë ¥ (GitHub ì›ë³¸ê³¼ ë™ì¼í•œ ë²„í¼ ì²˜ë¦¬)
                st.caption("ğŸ” Nova Pro ìƒì„¸ ë‹µë³€ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²° ì¤‘...")
                
                buffer_index = 0
                current_display = full_micro_response + "\n\n---\n\n**ğŸ” Nova Pro ìƒì„¸ ë¶„ì„:**\n\n"
                
                # GitHub ì›ë³¸ê³¼ ë™ì¼í•œ ë²„í¼ ì‹¤ì‹œê°„ ì¶œë ¥ ë°©ì‹
                while not pro_complete or buffer_index < len(pro_buffer):
                    # ë²„í¼ì— ìƒˆë¡œìš´ ë‚´ìš©ì´ ìˆìœ¼ë©´ ì¶œë ¥
                    while buffer_index < len(pro_buffer):
                        chunk = pro_buffer[buffer_index]
                        current_display += chunk
                        response_container.markdown(current_display + "â–Œ")
                        time.sleep(0.01)  # GitHub ì›ë³¸ê³¼ ë™ì¼í•œ ë¹ ë¥¸ íƒ€ì´í•‘ íš¨ê³¼
                        buffer_index += 1
                    
                    # Pro ëª¨ë¸ì´ ì•„ì§ ì™„ë£Œë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì ì‹œ ëŒ€ê¸° (GitHub ì›ë³¸ê³¼ ë™ì¼)
                    if not pro_complete:
                        time.sleep(0.05)
                
                # ìµœì¢… í‘œì‹œ (ì»¤ì„œ ì œê±°)
                response_container.markdown(current_display)
                
                # Pro ì‘ì—… ì™„ë£Œ ëŒ€ê¸° (GitHub ì›ë³¸ê³¼ ë™ì¼)
                future_pro.result()
                
                return current_display